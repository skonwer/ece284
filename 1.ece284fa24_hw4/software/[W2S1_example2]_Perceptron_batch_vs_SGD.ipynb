{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "miniature-siemens",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=2, out_features=1, bias=False)\n",
      ")\n",
      "[Parameter containing:\n",
      "tensor([[ 1., -1.]], requires_grad=True)]\n",
      "Epoch 0 - loss: 150.3333282470703\n",
      "Epoch 1 - loss: 6.120000839233398\n",
      "Epoch 2 - loss: 36.42483139038086\n",
      "Epoch 3 - loss: 18.104551315307617\n",
      "Epoch 4 - loss: 6.065778732299805\n",
      "Epoch 5 - loss: 10.36883544921875\n",
      "Epoch 6 - loss: 6.2322306632995605\n",
      "Epoch 7 - loss: 5.852260589599609\n",
      "Epoch 8 - loss: 6.088119983673096\n",
      "Epoch 9 - loss: 5.494865417480469\n",
      "Epoch 10 - loss: 5.574592590332031\n",
      "Epoch 11 - loss: 5.536750793457031\n",
      "Epoch 12 - loss: 5.476492404937744\n",
      "Epoch 13 - loss: 5.4938530921936035\n",
      "Epoch 14 - loss: 5.480627536773682\n",
      "Epoch 15 - loss: 5.477013111114502\n",
      "Epoch 16 - loss: 5.478647232055664\n",
      "Epoch 17 - loss: 5.476379871368408\n",
      "Epoch 18 - loss: 5.476480007171631\n",
      "Epoch 19 - loss: 5.4764628410339355\n",
      "Epoch 20 - loss: 5.47619104385376\n",
      "Epoch 21 - loss: 5.476251602172852\n",
      "Epoch 22 - loss: 5.476212978363037\n",
      "Epoch 23 - loss: 5.476190567016602\n",
      "Epoch 24 - loss: 5.476199626922607\n",
      "Epoch 25 - loss: 5.476192474365234\n",
      "Epoch 26 - loss: 5.476190567016602\n",
      "Epoch 27 - loss: 5.476191997528076\n",
      "Epoch 28 - loss: 5.476190090179443\n",
      "Epoch 29 - loss: 5.476190567016602\n",
      "Epoch 30 - loss: 5.476190090179443\n",
      "Epoch 31 - loss: 5.476190567016602\n",
      "Epoch 32 - loss: 5.47619104385376\n",
      "Epoch 33 - loss: 5.476190090179443\n",
      "Epoch 34 - loss: 5.476190090179443\n",
      "Epoch 35 - loss: 5.476190567016602\n",
      "Epoch 36 - loss: 5.476190090179443\n",
      "Epoch 37 - loss: 5.47619104385376\n",
      "Epoch 38 - loss: 5.47619104385376\n",
      "Epoch 39 - loss: 5.476190567016602\n",
      "Epoch 40 - loss: 5.47619104385376\n",
      "Epoch 41 - loss: 5.476188659667969\n",
      "Epoch 42 - loss: 5.476190090179443\n",
      "Epoch 43 - loss: 5.476189136505127\n",
      "Epoch 44 - loss: 5.476190090179443\n",
      "Epoch 45 - loss: 5.476191997528076\n",
      "Epoch 46 - loss: 5.476191997528076\n",
      "Epoch 47 - loss: 5.476190090179443\n",
      "Epoch 48 - loss: 5.476190567016602\n",
      "Epoch 49 - loss: 5.476190567016602\n",
      "Epoch 50 - loss: 5.476190090179443\n",
      "Epoch 51 - loss: 5.47619104385376\n",
      "Epoch 52 - loss: 5.476190567016602\n",
      "Epoch 53 - loss: 5.47619104385376\n",
      "Epoch 54 - loss: 5.47619104385376\n",
      "Epoch 55 - loss: 5.47619104385376\n",
      "Epoch 56 - loss: 5.476190567016602\n",
      "Epoch 57 - loss: 5.47619104385376\n",
      "Epoch 58 - loss: 5.47619104385376\n",
      "Epoch 59 - loss: 5.47619104385376\n",
      "Epoch 60 - loss: 5.476190567016602\n",
      "Epoch 61 - loss: 5.47619104385376\n",
      "Epoch 62 - loss: 5.47619104385376\n",
      "Epoch 63 - loss: 5.47619104385376\n",
      "Epoch 64 - loss: 5.476190567016602\n",
      "Epoch 65 - loss: 5.47619104385376\n",
      "Epoch 66 - loss: 5.47619104385376\n",
      "Epoch 67 - loss: 5.47619104385376\n",
      "Epoch 68 - loss: 5.476190567016602\n",
      "Epoch 69 - loss: 5.47619104385376\n",
      "Epoch 70 - loss: 5.47619104385376\n",
      "Epoch 71 - loss: 5.47619104385376\n",
      "Epoch 72 - loss: 5.476190567016602\n",
      "Epoch 73 - loss: 5.47619104385376\n",
      "Epoch 74 - loss: 5.47619104385376\n",
      "Epoch 75 - loss: 5.47619104385376\n",
      "Epoch 76 - loss: 5.476190567016602\n",
      "Epoch 77 - loss: 5.47619104385376\n",
      "Epoch 78 - loss: 5.47619104385376\n",
      "Epoch 79 - loss: 5.47619104385376\n",
      "Epoch 80 - loss: 5.476190567016602\n",
      "Epoch 81 - loss: 5.47619104385376\n",
      "Epoch 82 - loss: 5.47619104385376\n",
      "Epoch 83 - loss: 5.47619104385376\n",
      "Epoch 84 - loss: 5.476190567016602\n",
      "Epoch 85 - loss: 5.47619104385376\n",
      "Epoch 86 - loss: 5.47619104385376\n",
      "Epoch 87 - loss: 5.47619104385376\n",
      "Epoch 88 - loss: 5.476190567016602\n",
      "Epoch 89 - loss: 5.47619104385376\n",
      "Epoch 90 - loss: 5.47619104385376\n",
      "Epoch 91 - loss: 5.47619104385376\n",
      "Epoch 92 - loss: 5.476190567016602\n",
      "Epoch 93 - loss: 5.47619104385376\n",
      "Epoch 94 - loss: 5.47619104385376\n",
      "Epoch 95 - loss: 5.47619104385376\n",
      "Epoch 96 - loss: 5.476190567016602\n",
      "Epoch 97 - loss: 5.47619104385376\n",
      "Epoch 98 - loss: 5.47619104385376\n",
      "Epoch 99 - loss: 5.47619104385376\n",
      "Epoch 100 - loss: 5.476190567016602\n",
      "Epoch 101 - loss: 5.47619104385376\n",
      "Epoch 102 - loss: 5.47619104385376\n",
      "Epoch 103 - loss: 5.47619104385376\n",
      "Epoch 104 - loss: 5.476190567016602\n",
      "Epoch 105 - loss: 5.47619104385376\n",
      "Epoch 106 - loss: 5.47619104385376\n",
      "Epoch 107 - loss: 5.47619104385376\n",
      "Epoch 108 - loss: 5.476190567016602\n",
      "Epoch 109 - loss: 5.47619104385376\n",
      "Epoch 110 - loss: 5.47619104385376\n",
      "Epoch 111 - loss: 5.47619104385376\n",
      "Epoch 112 - loss: 5.476190567016602\n",
      "Epoch 113 - loss: 5.47619104385376\n",
      "Epoch 114 - loss: 5.47619104385376\n",
      "Epoch 115 - loss: 5.47619104385376\n",
      "Epoch 116 - loss: 5.476190567016602\n",
      "Epoch 117 - loss: 5.47619104385376\n",
      "Epoch 118 - loss: 5.47619104385376\n",
      "Epoch 119 - loss: 5.47619104385376\n",
      "Epoch 120 - loss: 5.476190567016602\n",
      "Epoch 121 - loss: 5.47619104385376\n",
      "Epoch 122 - loss: 5.47619104385376\n",
      "Epoch 123 - loss: 5.47619104385376\n",
      "Epoch 124 - loss: 5.476190567016602\n",
      "Epoch 125 - loss: 5.47619104385376\n",
      "Epoch 126 - loss: 5.47619104385376\n",
      "Epoch 127 - loss: 5.47619104385376\n",
      "Epoch 128 - loss: 5.476190567016602\n",
      "Epoch 129 - loss: 5.47619104385376\n",
      "Epoch 130 - loss: 5.47619104385376\n",
      "Epoch 131 - loss: 5.47619104385376\n",
      "Epoch 132 - loss: 5.476190567016602\n",
      "Epoch 133 - loss: 5.47619104385376\n",
      "Epoch 134 - loss: 5.47619104385376\n",
      "Epoch 135 - loss: 5.47619104385376\n",
      "Epoch 136 - loss: 5.476190567016602\n",
      "Epoch 137 - loss: 5.47619104385376\n",
      "Epoch 138 - loss: 5.47619104385376\n",
      "Epoch 139 - loss: 5.47619104385376\n",
      "Epoch 140 - loss: 5.476190567016602\n",
      "Epoch 141 - loss: 5.47619104385376\n",
      "Epoch 142 - loss: 5.47619104385376\n",
      "Epoch 143 - loss: 5.47619104385376\n",
      "Epoch 144 - loss: 5.476190567016602\n",
      "Epoch 145 - loss: 5.47619104385376\n",
      "Epoch 146 - loss: 5.47619104385376\n",
      "Epoch 147 - loss: 5.47619104385376\n",
      "Epoch 148 - loss: 5.476190567016602\n",
      "Epoch 149 - loss: 5.47619104385376\n",
      "Epoch 150 - loss: 5.47619104385376\n",
      "Epoch 151 - loss: 5.47619104385376\n",
      "Epoch 152 - loss: 5.476190567016602\n",
      "Epoch 153 - loss: 5.47619104385376\n",
      "Epoch 154 - loss: 5.47619104385376\n",
      "Epoch 155 - loss: 5.47619104385376\n",
      "Epoch 156 - loss: 5.476190567016602\n",
      "Epoch 157 - loss: 5.47619104385376\n",
      "Epoch 158 - loss: 5.47619104385376\n",
      "Epoch 159 - loss: 5.47619104385376\n",
      "Epoch 160 - loss: 5.476190567016602\n",
      "Epoch 161 - loss: 5.47619104385376\n",
      "Epoch 162 - loss: 5.47619104385376\n",
      "Epoch 163 - loss: 5.47619104385376\n",
      "Epoch 164 - loss: 5.476190567016602\n",
      "Epoch 165 - loss: 5.47619104385376\n",
      "Epoch 166 - loss: 5.47619104385376\n",
      "Epoch 167 - loss: 5.47619104385376\n",
      "Epoch 168 - loss: 5.476190567016602\n",
      "Epoch 169 - loss: 5.47619104385376\n",
      "Epoch 170 - loss: 5.47619104385376\n",
      "Epoch 171 - loss: 5.47619104385376\n",
      "Epoch 172 - loss: 5.476190567016602\n",
      "Epoch 173 - loss: 5.47619104385376\n",
      "Epoch 174 - loss: 5.47619104385376\n",
      "Epoch 175 - loss: 5.47619104385376\n",
      "Epoch 176 - loss: 5.476190567016602\n",
      "Epoch 177 - loss: 5.47619104385376\n",
      "Epoch 178 - loss: 5.47619104385376\n",
      "Epoch 179 - loss: 5.47619104385376\n",
      "Epoch 180 - loss: 5.476190567016602\n",
      "Epoch 181 - loss: 5.47619104385376\n",
      "Epoch 182 - loss: 5.47619104385376\n",
      "Epoch 183 - loss: 5.47619104385376\n",
      "Epoch 184 - loss: 5.476190567016602\n",
      "Epoch 185 - loss: 5.47619104385376\n",
      "Epoch 186 - loss: 5.47619104385376\n",
      "Epoch 187 - loss: 5.47619104385376\n",
      "Epoch 188 - loss: 5.476190567016602\n",
      "Epoch 189 - loss: 5.47619104385376\n",
      "Epoch 190 - loss: 5.47619104385376\n",
      "Epoch 191 - loss: 5.47619104385376\n",
      "Epoch 192 - loss: 5.476190567016602\n",
      "Epoch 193 - loss: 5.47619104385376\n",
      "Epoch 194 - loss: 5.47619104385376\n",
      "Epoch 195 - loss: 5.47619104385376\n",
      "Epoch 196 - loss: 5.476190567016602\n",
      "Epoch 197 - loss: 5.47619104385376\n",
      "Epoch 198 - loss: 5.47619104385376\n",
      "Epoch 199 - loss: 5.47619104385376\n",
      "Epoch 200 - loss: 5.476190567016602\n",
      "Epoch 201 - loss: 5.47619104385376\n",
      "Epoch 202 - loss: 5.47619104385376\n",
      "Epoch 203 - loss: 5.47619104385376\n",
      "Epoch 204 - loss: 5.476190567016602\n",
      "Epoch 205 - loss: 5.47619104385376\n",
      "Epoch 206 - loss: 5.47619104385376\n",
      "Epoch 207 - loss: 5.47619104385376\n",
      "Epoch 208 - loss: 5.476190567016602\n",
      "Epoch 209 - loss: 5.47619104385376\n",
      "Epoch 210 - loss: 5.47619104385376\n",
      "Epoch 211 - loss: 5.47619104385376\n",
      "Epoch 212 - loss: 5.476190567016602\n",
      "Epoch 213 - loss: 5.47619104385376\n",
      "Epoch 214 - loss: 5.47619104385376\n",
      "Epoch 215 - loss: 5.47619104385376\n",
      "Epoch 216 - loss: 5.476190567016602\n",
      "Epoch 217 - loss: 5.47619104385376\n",
      "Epoch 218 - loss: 5.47619104385376\n",
      "Epoch 219 - loss: 5.47619104385376\n",
      "Epoch 220 - loss: 5.476190567016602\n",
      "Epoch 221 - loss: 5.47619104385376\n",
      "Epoch 222 - loss: 5.47619104385376\n",
      "Epoch 223 - loss: 5.47619104385376\n",
      "Epoch 224 - loss: 5.476190567016602\n",
      "Epoch 225 - loss: 5.47619104385376\n",
      "Epoch 226 - loss: 5.47619104385376\n",
      "Epoch 227 - loss: 5.47619104385376\n",
      "Epoch 228 - loss: 5.476190567016602\n",
      "Epoch 229 - loss: 5.47619104385376\n",
      "Epoch 230 - loss: 5.47619104385376\n",
      "Epoch 231 - loss: 5.47619104385376\n",
      "Epoch 232 - loss: 5.476190567016602\n",
      "Epoch 233 - loss: 5.47619104385376\n",
      "Epoch 234 - loss: 5.47619104385376\n",
      "Epoch 235 - loss: 5.47619104385376\n",
      "Epoch 236 - loss: 5.476190567016602\n",
      "Epoch 237 - loss: 5.47619104385376\n",
      "Epoch 238 - loss: 5.47619104385376\n",
      "Epoch 239 - loss: 5.47619104385376\n",
      "Epoch 240 - loss: 5.476190567016602\n",
      "Epoch 241 - loss: 5.47619104385376\n",
      "Epoch 242 - loss: 5.47619104385376\n",
      "Epoch 243 - loss: 5.47619104385376\n",
      "Epoch 244 - loss: 5.476190567016602\n",
      "Epoch 245 - loss: 5.47619104385376\n",
      "Epoch 246 - loss: 5.47619104385376\n",
      "Epoch 247 - loss: 5.47619104385376\n",
      "Epoch 248 - loss: 5.476190567016602\n",
      "Epoch 249 - loss: 5.47619104385376\n",
      "Epoch 250 - loss: 5.47619104385376\n",
      "Epoch 251 - loss: 5.47619104385376\n",
      "Epoch 252 - loss: 5.476190567016602\n",
      "Epoch 253 - loss: 5.47619104385376\n",
      "Epoch 254 - loss: 5.47619104385376\n",
      "Epoch 255 - loss: 5.47619104385376\n",
      "Epoch 256 - loss: 5.476190567016602\n",
      "Epoch 257 - loss: 5.47619104385376\n",
      "Epoch 258 - loss: 5.47619104385376\n",
      "Epoch 259 - loss: 5.47619104385376\n",
      "Epoch 260 - loss: 5.476190567016602\n",
      "Epoch 261 - loss: 5.47619104385376\n",
      "Epoch 262 - loss: 5.47619104385376\n",
      "Epoch 263 - loss: 5.47619104385376\n",
      "Epoch 264 - loss: 5.476190567016602\n",
      "Epoch 265 - loss: 5.47619104385376\n",
      "Epoch 266 - loss: 5.47619104385376\n",
      "Epoch 267 - loss: 5.47619104385376\n",
      "Epoch 268 - loss: 5.476190567016602\n",
      "Epoch 269 - loss: 5.47619104385376\n",
      "Epoch 270 - loss: 5.47619104385376\n",
      "Epoch 271 - loss: 5.47619104385376\n",
      "Epoch 272 - loss: 5.476190567016602\n",
      "Epoch 273 - loss: 5.47619104385376\n",
      "Epoch 274 - loss: 5.47619104385376\n",
      "Epoch 275 - loss: 5.47619104385376\n",
      "Epoch 276 - loss: 5.476190567016602\n",
      "Epoch 277 - loss: 5.47619104385376\n",
      "Epoch 278 - loss: 5.47619104385376\n",
      "Epoch 279 - loss: 5.47619104385376\n",
      "Epoch 280 - loss: 5.476190567016602\n",
      "Epoch 281 - loss: 5.47619104385376\n",
      "Epoch 282 - loss: 5.47619104385376\n",
      "Epoch 283 - loss: 5.47619104385376\n",
      "Epoch 284 - loss: 5.476190567016602\n",
      "Epoch 285 - loss: 5.47619104385376\n",
      "Epoch 286 - loss: 5.47619104385376\n",
      "Epoch 287 - loss: 5.47619104385376\n",
      "Epoch 288 - loss: 5.476190567016602\n",
      "Epoch 289 - loss: 5.47619104385376\n",
      "Epoch 290 - loss: 5.47619104385376\n",
      "Epoch 291 - loss: 5.47619104385376\n",
      "Epoch 292 - loss: 5.476190567016602\n",
      "Epoch 293 - loss: 5.47619104385376\n",
      "Epoch 294 - loss: 5.47619104385376\n",
      "Epoch 295 - loss: 5.47619104385376\n",
      "Epoch 296 - loss: 5.476190567016602\n",
      "Epoch 297 - loss: 5.47619104385376\n",
      "Epoch 298 - loss: 5.47619104385376\n",
      "Epoch 299 - loss: 5.47619104385376\n",
      "when x = tensor([1., 3.]), y = tensor([3.5714], grad_fn=<SqueezeBackward3>)\n",
      "when x = tensor([2., 6.]), y = tensor([7.1429], grad_fn=<SqueezeBackward3>)\n",
      "when x = tensor([3., 9.]), y = tensor([10.7143], grad_fn=<SqueezeBackward3>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc880a62d60>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2,1,bias=False)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "\n",
    "net.fc1.weight = torch.nn.Parameter(torch.tensor([[1., -1.]], requires_grad=True))\n",
    "\n",
    "print(list(net.parameters()))\n",
    "\n",
    "#input = torch.randn(1,2)\n",
    "#out = net(input)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "#def criterion(out, label):\n",
    "#    return ((label - out)**2).mean()\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.5)\n",
    "#optimizer = optim.Adam(net.parameters(), lr=0.005)\n",
    "\n",
    "\n",
    "data = torch.tensor([[1.,3.], [2.,6.], [3.,9.]], dtype=torch.float)\n",
    "target = torch.tensor([[1.],[5.],[13.]], dtype=torch.float)\n",
    "\n",
    "hist = []\n",
    "\n",
    "############## Batch GD based update ##############       \n",
    "      \n",
    "for epoch in range(300):   \n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(data)\n",
    "    loss = criterion(outputs, target)\n",
    "    loss.backward()\n",
    "    hist.append(loss.detach())\n",
    "    optimizer.step()\n",
    "    print(\"Epoch {} - loss: {}\".format(epoch, loss))\n",
    "####################################################\n",
    "\n",
    "### Test the trained network ###\n",
    "for i, current_data in enumerate(data):\n",
    "    out = net(current_data)  \n",
    "    print(\"when x = {}, y = {}\".format(current_data, out))\n",
    "    \n",
    "plt.plot(hist, label = \"training curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "functional-booking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=2, out_features=1, bias=False)\n",
      ")\n",
      "[Parameter containing:\n",
      "tensor([[ 1., -1.]], requires_grad=True)]\n",
      "Epoch 0 - loss: 150.3333282470703\n",
      "Epoch 1 - loss: 6.120000839233398\n",
      "Epoch 2 - loss: 36.42483139038086\n",
      "Epoch 3 - loss: 18.104551315307617\n",
      "Epoch 4 - loss: 6.065778732299805\n",
      "Epoch 5 - loss: 10.36883544921875\n",
      "Epoch 6 - loss: 6.2322306632995605\n",
      "Epoch 7 - loss: 5.852260589599609\n",
      "Epoch 8 - loss: 6.088119983673096\n",
      "Epoch 9 - loss: 5.494865417480469\n",
      "Epoch 10 - loss: 5.574592590332031\n",
      "Epoch 11 - loss: 5.536750793457031\n",
      "Epoch 12 - loss: 5.476492404937744\n",
      "Epoch 13 - loss: 5.4938530921936035\n",
      "Epoch 14 - loss: 5.480627536773682\n",
      "Epoch 15 - loss: 5.477013111114502\n",
      "Epoch 16 - loss: 5.478647232055664\n",
      "Epoch 17 - loss: 5.476379871368408\n",
      "Epoch 18 - loss: 5.476480007171631\n",
      "Epoch 19 - loss: 5.4764628410339355\n",
      "Epoch 20 - loss: 5.47619104385376\n",
      "Epoch 21 - loss: 5.476251602172852\n",
      "Epoch 22 - loss: 5.476212978363037\n",
      "Epoch 23 - loss: 5.476190567016602\n",
      "Epoch 24 - loss: 5.476199626922607\n",
      "Epoch 25 - loss: 5.476192474365234\n",
      "Epoch 26 - loss: 5.476190567016602\n",
      "Epoch 27 - loss: 5.476191997528076\n",
      "Epoch 28 - loss: 5.476190090179443\n",
      "Epoch 29 - loss: 5.476190567016602\n",
      "Epoch 30 - loss: 5.476190090179443\n",
      "Epoch 31 - loss: 5.476190567016602\n",
      "Epoch 32 - loss: 5.47619104385376\n",
      "Epoch 33 - loss: 5.476190090179443\n",
      "Epoch 34 - loss: 5.476190090179443\n",
      "Epoch 35 - loss: 5.476190567016602\n",
      "Epoch 36 - loss: 5.476190090179443\n",
      "Epoch 37 - loss: 5.47619104385376\n",
      "Epoch 38 - loss: 5.47619104385376\n",
      "Epoch 39 - loss: 5.476190567016602\n",
      "Epoch 40 - loss: 5.47619104385376\n",
      "Epoch 41 - loss: 5.476188659667969\n",
      "Epoch 42 - loss: 5.476190090179443\n",
      "Epoch 43 - loss: 5.476189136505127\n",
      "Epoch 44 - loss: 5.476190090179443\n",
      "Epoch 45 - loss: 5.476191997528076\n",
      "Epoch 46 - loss: 5.476191997528076\n",
      "Epoch 47 - loss: 5.476190090179443\n",
      "Epoch 48 - loss: 5.476190567016602\n",
      "Epoch 49 - loss: 5.476190567016602\n",
      "Epoch 50 - loss: 5.476190090179443\n",
      "Epoch 51 - loss: 5.47619104385376\n",
      "Epoch 52 - loss: 5.476190567016602\n",
      "Epoch 53 - loss: 5.47619104385376\n",
      "Epoch 54 - loss: 5.47619104385376\n",
      "Epoch 55 - loss: 5.47619104385376\n",
      "Epoch 56 - loss: 5.476190567016602\n",
      "Epoch 57 - loss: 5.47619104385376\n",
      "Epoch 58 - loss: 5.47619104385376\n",
      "Epoch 59 - loss: 5.47619104385376\n",
      "Epoch 60 - loss: 5.476190567016602\n",
      "Epoch 61 - loss: 5.47619104385376\n",
      "Epoch 62 - loss: 5.47619104385376\n",
      "Epoch 63 - loss: 5.47619104385376\n",
      "Epoch 64 - loss: 5.476190567016602\n",
      "Epoch 65 - loss: 5.47619104385376\n",
      "Epoch 66 - loss: 5.47619104385376\n",
      "Epoch 67 - loss: 5.47619104385376\n",
      "Epoch 68 - loss: 5.476190567016602\n",
      "Epoch 69 - loss: 5.47619104385376\n",
      "Epoch 70 - loss: 5.47619104385376\n",
      "Epoch 71 - loss: 5.47619104385376\n",
      "Epoch 72 - loss: 5.476190567016602\n",
      "Epoch 73 - loss: 5.47619104385376\n",
      "Epoch 74 - loss: 5.47619104385376\n",
      "Epoch 75 - loss: 5.47619104385376\n",
      "Epoch 76 - loss: 5.476190567016602\n",
      "Epoch 77 - loss: 5.47619104385376\n",
      "Epoch 78 - loss: 5.47619104385376\n",
      "Epoch 79 - loss: 5.47619104385376\n",
      "Epoch 80 - loss: 5.476190567016602\n",
      "Epoch 81 - loss: 5.47619104385376\n",
      "Epoch 82 - loss: 5.47619104385376\n",
      "Epoch 83 - loss: 5.47619104385376\n",
      "Epoch 84 - loss: 5.476190567016602\n",
      "Epoch 85 - loss: 5.47619104385376\n",
      "Epoch 86 - loss: 5.47619104385376\n",
      "Epoch 87 - loss: 5.47619104385376\n",
      "Epoch 88 - loss: 5.476190567016602\n",
      "Epoch 89 - loss: 5.47619104385376\n",
      "Epoch 90 - loss: 5.47619104385376\n",
      "Epoch 91 - loss: 5.47619104385376\n",
      "Epoch 92 - loss: 5.476190567016602\n",
      "Epoch 93 - loss: 5.47619104385376\n",
      "Epoch 94 - loss: 5.47619104385376\n",
      "Epoch 95 - loss: 5.47619104385376\n",
      "Epoch 96 - loss: 5.476190567016602\n",
      "Epoch 97 - loss: 5.47619104385376\n",
      "Epoch 98 - loss: 5.47619104385376\n",
      "Epoch 99 - loss: 5.47619104385376\n",
      "Epoch 100 - loss: 5.476190567016602\n",
      "Epoch 101 - loss: 5.47619104385376\n",
      "Epoch 102 - loss: 5.47619104385376\n",
      "Epoch 103 - loss: 5.47619104385376\n",
      "Epoch 104 - loss: 5.476190567016602\n",
      "Epoch 105 - loss: 5.47619104385376\n",
      "Epoch 106 - loss: 5.47619104385376\n",
      "Epoch 107 - loss: 5.47619104385376\n",
      "Epoch 108 - loss: 5.476190567016602\n",
      "Epoch 109 - loss: 5.47619104385376\n",
      "Epoch 110 - loss: 5.47619104385376\n",
      "Epoch 111 - loss: 5.47619104385376\n",
      "Epoch 112 - loss: 5.476190567016602\n",
      "Epoch 113 - loss: 5.47619104385376\n",
      "Epoch 114 - loss: 5.47619104385376\n",
      "Epoch 115 - loss: 5.47619104385376\n",
      "Epoch 116 - loss: 5.476190567016602\n",
      "Epoch 117 - loss: 5.47619104385376\n",
      "Epoch 118 - loss: 5.47619104385376\n",
      "Epoch 119 - loss: 5.47619104385376\n",
      "Epoch 120 - loss: 5.476190567016602\n",
      "Epoch 121 - loss: 5.47619104385376\n",
      "Epoch 122 - loss: 5.47619104385376\n",
      "Epoch 123 - loss: 5.47619104385376\n",
      "Epoch 124 - loss: 5.476190567016602\n",
      "Epoch 125 - loss: 5.47619104385376\n",
      "Epoch 126 - loss: 5.47619104385376\n",
      "Epoch 127 - loss: 5.47619104385376\n",
      "Epoch 128 - loss: 5.476190567016602\n",
      "Epoch 129 - loss: 5.47619104385376\n",
      "Epoch 130 - loss: 5.47619104385376\n",
      "Epoch 131 - loss: 5.47619104385376\n",
      "Epoch 132 - loss: 5.476190567016602\n",
      "Epoch 133 - loss: 5.47619104385376\n",
      "Epoch 134 - loss: 5.47619104385376\n",
      "Epoch 135 - loss: 5.47619104385376\n",
      "Epoch 136 - loss: 5.476190567016602\n",
      "Epoch 137 - loss: 5.47619104385376\n",
      "Epoch 138 - loss: 5.47619104385376\n",
      "Epoch 139 - loss: 5.47619104385376\n",
      "Epoch 140 - loss: 5.476190567016602\n",
      "Epoch 141 - loss: 5.47619104385376\n",
      "Epoch 142 - loss: 5.47619104385376\n",
      "Epoch 143 - loss: 5.47619104385376\n",
      "Epoch 144 - loss: 5.476190567016602\n",
      "Epoch 145 - loss: 5.47619104385376\n",
      "Epoch 146 - loss: 5.47619104385376\n",
      "Epoch 147 - loss: 5.47619104385376\n",
      "Epoch 148 - loss: 5.476190567016602\n",
      "Epoch 149 - loss: 5.47619104385376\n",
      "Epoch 150 - loss: 5.47619104385376\n",
      "Epoch 151 - loss: 5.47619104385376\n",
      "Epoch 152 - loss: 5.476190567016602\n",
      "Epoch 153 - loss: 5.47619104385376\n",
      "Epoch 154 - loss: 5.47619104385376\n",
      "Epoch 155 - loss: 5.47619104385376\n",
      "Epoch 156 - loss: 5.476190567016602\n",
      "Epoch 157 - loss: 5.47619104385376\n",
      "Epoch 158 - loss: 5.47619104385376\n",
      "Epoch 159 - loss: 5.47619104385376\n",
      "Epoch 160 - loss: 5.476190567016602\n",
      "Epoch 161 - loss: 5.47619104385376\n",
      "Epoch 162 - loss: 5.47619104385376\n",
      "Epoch 163 - loss: 5.47619104385376\n",
      "Epoch 164 - loss: 5.476190567016602\n",
      "Epoch 165 - loss: 5.47619104385376\n",
      "Epoch 166 - loss: 5.47619104385376\n",
      "Epoch 167 - loss: 5.47619104385376\n",
      "Epoch 168 - loss: 5.476190567016602\n",
      "Epoch 169 - loss: 5.47619104385376\n",
      "Epoch 170 - loss: 5.47619104385376\n",
      "Epoch 171 - loss: 5.47619104385376\n",
      "Epoch 172 - loss: 5.476190567016602\n",
      "Epoch 173 - loss: 5.47619104385376\n",
      "Epoch 174 - loss: 5.47619104385376\n",
      "Epoch 175 - loss: 5.47619104385376\n",
      "Epoch 176 - loss: 5.476190567016602\n",
      "Epoch 177 - loss: 5.47619104385376\n",
      "Epoch 178 - loss: 5.47619104385376\n",
      "Epoch 179 - loss: 5.47619104385376\n",
      "Epoch 180 - loss: 5.476190567016602\n",
      "Epoch 181 - loss: 5.47619104385376\n",
      "Epoch 182 - loss: 5.47619104385376\n",
      "Epoch 183 - loss: 5.47619104385376\n",
      "Epoch 184 - loss: 5.476190567016602\n",
      "Epoch 185 - loss: 5.47619104385376\n",
      "Epoch 186 - loss: 5.47619104385376\n",
      "Epoch 187 - loss: 5.47619104385376\n",
      "Epoch 188 - loss: 5.476190567016602\n",
      "Epoch 189 - loss: 5.47619104385376\n",
      "Epoch 190 - loss: 5.47619104385376\n",
      "Epoch 191 - loss: 5.47619104385376\n",
      "Epoch 192 - loss: 5.476190567016602\n",
      "Epoch 193 - loss: 5.47619104385376\n",
      "Epoch 194 - loss: 5.47619104385376\n",
      "Epoch 195 - loss: 5.47619104385376\n",
      "Epoch 196 - loss: 5.476190567016602\n",
      "Epoch 197 - loss: 5.47619104385376\n",
      "Epoch 198 - loss: 5.47619104385376\n",
      "Epoch 199 - loss: 5.47619104385376\n",
      "Epoch 200 - loss: 5.476190567016602\n",
      "Epoch 201 - loss: 5.47619104385376\n",
      "Epoch 202 - loss: 5.47619104385376\n",
      "Epoch 203 - loss: 5.47619104385376\n",
      "Epoch 204 - loss: 5.476190567016602\n",
      "Epoch 205 - loss: 5.47619104385376\n",
      "Epoch 206 - loss: 5.47619104385376\n",
      "Epoch 207 - loss: 5.47619104385376\n",
      "Epoch 208 - loss: 5.476190567016602\n",
      "Epoch 209 - loss: 5.47619104385376\n",
      "Epoch 210 - loss: 5.47619104385376\n",
      "Epoch 211 - loss: 5.47619104385376\n",
      "Epoch 212 - loss: 5.476190567016602\n",
      "Epoch 213 - loss: 5.47619104385376\n",
      "Epoch 214 - loss: 5.47619104385376\n",
      "Epoch 215 - loss: 5.47619104385376\n",
      "Epoch 216 - loss: 5.476190567016602\n",
      "Epoch 217 - loss: 5.47619104385376\n",
      "Epoch 218 - loss: 5.47619104385376\n",
      "Epoch 219 - loss: 5.47619104385376\n",
      "Epoch 220 - loss: 5.476190567016602\n",
      "Epoch 221 - loss: 5.47619104385376\n",
      "Epoch 222 - loss: 5.47619104385376\n",
      "Epoch 223 - loss: 5.47619104385376\n",
      "Epoch 224 - loss: 5.476190567016602\n",
      "Epoch 225 - loss: 5.47619104385376\n",
      "Epoch 226 - loss: 5.47619104385376\n",
      "Epoch 227 - loss: 5.47619104385376\n",
      "Epoch 228 - loss: 5.476190567016602\n",
      "Epoch 229 - loss: 5.47619104385376\n",
      "Epoch 230 - loss: 5.47619104385376\n",
      "Epoch 231 - loss: 5.47619104385376\n",
      "Epoch 232 - loss: 5.476190567016602\n",
      "Epoch 233 - loss: 5.47619104385376\n",
      "Epoch 234 - loss: 5.47619104385376\n",
      "Epoch 235 - loss: 5.47619104385376\n",
      "Epoch 236 - loss: 5.476190567016602\n",
      "Epoch 237 - loss: 5.47619104385376\n",
      "Epoch 238 - loss: 5.47619104385376\n",
      "Epoch 239 - loss: 5.47619104385376\n",
      "Epoch 240 - loss: 5.476190567016602\n",
      "Epoch 241 - loss: 5.47619104385376\n",
      "Epoch 242 - loss: 5.47619104385376\n",
      "Epoch 243 - loss: 5.47619104385376\n",
      "Epoch 244 - loss: 5.476190567016602\n",
      "Epoch 245 - loss: 5.47619104385376\n",
      "Epoch 246 - loss: 5.47619104385376\n",
      "Epoch 247 - loss: 5.47619104385376\n",
      "Epoch 248 - loss: 5.476190567016602\n",
      "Epoch 249 - loss: 5.47619104385376\n",
      "Epoch 250 - loss: 5.47619104385376\n",
      "Epoch 251 - loss: 5.47619104385376\n",
      "Epoch 252 - loss: 5.476190567016602\n",
      "Epoch 253 - loss: 5.47619104385376\n",
      "Epoch 254 - loss: 5.47619104385376\n",
      "Epoch 255 - loss: 5.47619104385376\n",
      "Epoch 256 - loss: 5.476190567016602\n",
      "Epoch 257 - loss: 5.47619104385376\n",
      "Epoch 258 - loss: 5.47619104385376\n",
      "Epoch 259 - loss: 5.47619104385376\n",
      "Epoch 260 - loss: 5.476190567016602\n",
      "Epoch 261 - loss: 5.47619104385376\n",
      "Epoch 262 - loss: 5.47619104385376\n",
      "Epoch 263 - loss: 5.47619104385376\n",
      "Epoch 264 - loss: 5.476190567016602\n",
      "Epoch 265 - loss: 5.47619104385376\n",
      "Epoch 266 - loss: 5.47619104385376\n",
      "Epoch 267 - loss: 5.47619104385376\n",
      "Epoch 268 - loss: 5.476190567016602\n",
      "Epoch 269 - loss: 5.47619104385376\n",
      "Epoch 270 - loss: 5.47619104385376\n",
      "Epoch 271 - loss: 5.47619104385376\n",
      "Epoch 272 - loss: 5.476190567016602\n",
      "Epoch 273 - loss: 5.47619104385376\n",
      "Epoch 274 - loss: 5.47619104385376\n",
      "Epoch 275 - loss: 5.47619104385376\n",
      "Epoch 276 - loss: 5.476190567016602\n",
      "Epoch 277 - loss: 5.47619104385376\n",
      "Epoch 278 - loss: 5.47619104385376\n",
      "Epoch 279 - loss: 5.47619104385376\n",
      "Epoch 280 - loss: 5.476190567016602\n",
      "Epoch 281 - loss: 5.47619104385376\n",
      "Epoch 282 - loss: 5.47619104385376\n",
      "Epoch 283 - loss: 5.47619104385376\n",
      "Epoch 284 - loss: 5.476190567016602\n",
      "Epoch 285 - loss: 5.47619104385376\n",
      "Epoch 286 - loss: 5.47619104385376\n",
      "Epoch 287 - loss: 5.47619104385376\n",
      "Epoch 288 - loss: 5.476190567016602\n",
      "Epoch 289 - loss: 5.47619104385376\n",
      "Epoch 290 - loss: 5.47619104385376\n",
      "Epoch 291 - loss: 5.47619104385376\n",
      "Epoch 292 - loss: 5.476190567016602\n",
      "Epoch 293 - loss: 5.47619104385376\n",
      "Epoch 294 - loss: 5.47619104385376\n",
      "Epoch 295 - loss: 5.47619104385376\n",
      "Epoch 296 - loss: 5.476190567016602\n",
      "Epoch 297 - loss: 5.47619104385376\n",
      "Epoch 298 - loss: 5.47619104385376\n",
      "Epoch 299 - loss: 5.47619104385376\n",
      "when x = tensor([1., 3.]), y = tensor([3.5714], grad_fn=<SqueezeBackward3>)\n",
      "when x = tensor([2., 6.]), y = tensor([7.1429], grad_fn=<SqueezeBackward3>)\n",
      "when x = tensor([3., 9.]), y = tensor([10.7143], grad_fn=<SqueezeBackward3>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-eeb08814664f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"when x = {}, y = {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"training curve\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2838\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2839\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2840\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2841\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1741\u001b[0m         \"\"\"\n\u001b[1;32m   1742\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mindex_of\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m   1641\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1642\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1643\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1644\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVisibleDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \u001b[0;31m# NumPy 1.19 will warn on ragged input, and we can't actually use it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_check_1d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[0;34m\"\"\"Convert scalars to 1d arrays; pass-through arrays as is.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevant_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-packet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-survivor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-light",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-aviation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-buyer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
